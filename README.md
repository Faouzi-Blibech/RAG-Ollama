# Local RAG System with LLM Integration

A Retrieval-Augmented Generation (RAG) system that combines document ingestion, vector search, and Large Language Model capabilities for intelligent question-answering from local documents.

## ğŸŒŸ Features

- **Dual LLM Support**: Local Ollama models and remote API integration (Kimi/Moonshot)
- **Smart Document Processing**: PDF to text conversion with PyMuPDF
- **Adaptive Chunking**: Automatic text segmentation based on document size
- **Vector Search**: Semantic search using sentence transformers and SQLite-Vec
- **Web Interface**: Modern dark-themed frontend with file upload
- **Performance Optimization**: WAL mode, RAM optimization, and comprehensive benchmarking
- **Query History**: Persistent storage and management of Q&A sessions
- **Document Summarization**: AI-powered book/document summaries

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚    Backend      â”‚    â”‚   Local LLM     â”‚
â”‚   (HTML/JS)     â”‚â—„â”€â”€â–ºâ”‚   (Flask)       â”‚â—„â”€â”€â–ºâ”‚   (Ollama)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   SQLite + Vec  â”‚
                        â”‚   (Embeddings)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Ollama (for local LLM)
- SQLite with vector extension

### Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/rag-system
cd rag-system
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Start Ollama service:
```bash
ollama serve
ollama pull llama3.1:8b
```

4. Run the application:
```bash
# Backend
python backend.py

# CLI version
python test2.py
```

5. Access web interface at `http://localhost:5000`

## ğŸ“ Project Structure

```
WORKINGRAG/
â”œâ”€â”€ backend.py          # Flask web server
â”œâ”€â”€ frontend.html       # Web interface
â”œâ”€â”€ test2.py           # Main CLI application (Local Ollama)
â”œâ”€â”€ data/              # Document storage
â”œâ”€â”€ my_docs.db         # Vector database
â”œâ”€â”€ queries.db         # Query history
â””â”€â”€ log.txt           # Performance benchmarks
```

## ğŸ”§ Configuration

Key configuration options in `test2.py`:

```python
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "llama3.1:8b"
TOP_K = 3  # Number of retrieved chunks
USE_RAM = bool(os.getenv("RAM", ""))  # In-memory database
```

## ğŸ“Š Performance Features

- **Benchmark Logging**: Detailed timing for each operation
- **Database Optimization**: WAL mode and memory storage options
- **Adaptive Chunking**: Dynamic chunk sizing based on document length
- **Efficient Retrieval**: Vector similarity search with configurable TOP_K

## ğŸ¯ Usage Examples

### CLI Interface
```bash
python test2.py
> upload  # Select PDF/TXT file via dialog
> What is the main topic of this document?
> summarize filename.pdf
> history  # View past queries
```

### Web Interface
1. Upload documents via the web interface
2. Ask questions in natural language
3. View query history in the sidebar
4. Generate document summaries

## ğŸ› ï¸ Technical Implementation

### Vector Embeddings
- **Model**: `all-MiniLM-L6-v2` (384 dimensions)
- **Storage**: SQLite with vec0 extension
- **Similarity**: Cosine similarity search

### Document Processing
- **PDF Support**: PyMuPDF for text extraction
- **Text Chunking**: Paragraph-aware adaptive chunking
- **Encoding**: UTF-8 with error handling

### LLM Integration
- **Local**: Ollama with Llama 3.1 8B
- **Remote**: Kimi/Moonshot API support
- **Fallback**: Graceful error handling

## ğŸ“ˆ Performance Metrics

The system logs detailed performance metrics to `log.txt`:
- Document ingestion time
- Embedding generation time
- Vector search latency
- LLM response time

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- Sentence Transformers for embedding models
- SQLite-Vec for vector search capabilities
- Ollama for local LLM hosting
- PyMuPDF for PDF processing

---

Built during internship at Aptum - July 2025

